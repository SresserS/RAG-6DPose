<!DOCTYPE html>
<html>

<head>
    <meta charset="utf-8">
    <meta name="description" content="RAG-6DPose: Retrieval-Augmented 6D Pose Estimation via Leveraging CAD as Knowledge Base">
    <meta name="keywords" content="6D Pose Estimation, Retrieval-Augmented Generation">
    <meta name="viewport" content="width=device-width, initial-scale=1">
    <title>RAG-6DPose: Retrieval-Augmented 6D Pose Estimation via Leveraging CAD as Knowledge Base</title>

    <link href="https://fonts.googleapis.com/css?family=Google+Sans|Noto+Sans|Castoro" rel="stylesheet">

    <link rel="stylesheet" href="./static/css/bulma.min.css">
    <link rel="stylesheet" href="./static/css/bulma-carousel.min.css">
    <link rel="stylesheet" href="./static/css/bulma-slider.min.css">
    <link rel="stylesheet" href="./static/css/fontawesome.all.min.css">
    <link rel="stylesheet" href="https://cdn.jsdelivr.net/gh/jpswalsh/academicons@1/css/academicons.min.css">
    <link rel="stylesheet" href="./static/css/index.css">

    <script src="https://ajax.googleapis.com/ajax/libs/jquery/3.5.1/jquery.min.js"></script>
    <script defer src="./static/js/fontawesome.all.min.js"></script>
    <script src="./static/js/bulma-carousel.min.js"></script>
    <script src="./static/js/bulma-slider.min.js"></script>
    <script src="./static/js/index.js"></script>
</head>

<body>

    <section class="hero">
        <div class="hero-body">
            <div class="container is-max-desktop">
                <div class="columns is-centered">
                    <div class="column has-text-centered">
                        <h1 class="title is-1 publication-title">RAG-6DPose: Retrieval-Augmented 6D Pose Estimation via Leveraging CAD as Knowledge Base</h1>
                        <div class="is-size-5 publication-authors">
                            <span class="author-block">
                                Kuanning Wang<sup>1</sup>,
                                Yuqian Fu<sup>2</sup>,
                                Tianyu Wang<sup>1</sup>,
                                Yanwei Fu<sup>1</sup>,
                                Longfei Liang<sup>3</sup>,
                                Yu-Gang Jiang<sup>1</sup>,
                                Xiangyang Xue<sup>1</sup>,

                            </span>
                        </div>

                        <div class="is-size-5 publication-authors">
                            <sup>1</sup><span class="author-block">Fudan University</span>,
                            <sup>2</sup><span class="author-block">INSAIT, Sofia University</span>,
                            <sup>3</sup><span class="author-block">NeuhHelium Co.,Ltd.</span>,
                        </div>


                        <div class="is-size-5 publication-authors">
                            <span class="author-block">Under review for IROS 2025</span>
                        </div>


                        <div class="column has-text-centered">
                            <div class="publication-links">

                                <span class="link-block">
                                    <a href="#" class="external-link button is-normal is-rounded is-dark">
                                        <span class="icon">
                                            <i class="fas fa-file-pdf"></i>
                                        </span>
                                        <span>Paper (Comming Soon)</span>
                                    </a>
                                </span>
                                <span class="link-block">
                                    <a href="#" class="external-link button is-normal is-rounded is-dark">
                                        <span class="icon">
                                            <i class="ai ai-arxiv"></i>
                                        </span>
                                        <span>arXiv (Comming Soon)</span>
                                    </a>
                                </span>

                                <span class="link-block">
                                    <a href="#" class="external-link button is-normal is-rounded is-dark">
                                        <span class="icon">
                                            <i class="fab fa-github"></i>
                                        </span>
                                        <span>Code (Comming Soon)</span>
                                    </a>
                                </span>

                            </div>
                        </div>

                    </div>
                </div>
            </div>
        </div>
    </section>

    <section class="section">
        <div class="container is-max-desktop">
          <!-- Abstract. -->
          <div class="columns is-centered has-text-centered">
            <div class="column is-four-fifths">
              <h2 class="title is-3">Abstract</h2>
              <div class="content has-text-justified">
                <p>
                    Accurate 6D pose estimation is key for robotic manipulation, enabling precise object localization for tasks like grasping. We present \textit{RAG-6DPose}, a retrieval-augmented approach that leverages 3D CAD models as a knowledge base by integrating both visual and geometric cues. Our RAG-6DPose roughly contains three stages: 
                    1) Building a Multi-Modal CAD Knowledge Base by extracting 2D visual features from multi-view CAD rendered images and also attaching 3D points;
                    2) Retrieving relevant CAD features from the knowledge base based on the current query image via our ReSPC module; and 3) Incorporating retrieved CAD information to refine pose predictions via retrieval-augmented decoding. Experimental results on standard benchmarks and real-world robotic tasks demonstrate the effectiveness and robustness of our approach, particularly in handling occlusions and novel viewpoints.
                </p>
              </div>
            </div>
          </div>
          <!--/ Abstract. -->
      
        </div>
      </section>

      <section class="section">
        <div class="container is-max-desktop">
          <!-- Abstract. -->
          <div class="columns is-centered has-text-centered">
            <div class="column is-four-fifths">
              <h2 class="title is-3">Computational Overhead</h2>
              <div class="content has-text-justified">
                <p>
                    DINOv2 uses FlashAttention to speed up transformers. The CAD knowledge base is compact, with most parameters outside the knowledge base shared across objects. Foundation models also help reduce training time.
                    Our network has two components: the Encoder-Decoder, which generates query features, and the Key Feature Extraction network, which creates key features and is saved locally.
                    Inference times for a single object on the TUD-L dataset are 65ms for the Encoder-Decoder, compared to 61ms for MRCNet. Our model improves accuracy with minimal extra computational cost.
                    Compared to SurfEmb, our total time, including PnP-RANSAC, is similar, but performance is significantly better. As with other advanced methods like 3DNEL, some external time is always required, but we aim to optimize this further.
                <p>
              </div>
            </div>
          </div>
          <!--/ Abstract. -->
      
        </div>
      </section>
      


    <footer class="footer">
        <div class="container">
            <div class="columns is-centered">
                <div class="column is-8">
                    <div class="content">
                        <p>
                            This website template is borrowed from <a
                                href="https://github.com/nerfies/nerfies.github.io">nerfies.github.io</a>.
                        </p>
                    </div>
                </div>
            </div>
        </div>
    </footer>

</body>

</html>